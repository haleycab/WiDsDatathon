{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Explore the Data\n",
    "\n",
    "From the `TRAIN_NEW` folder, we will load:\n",
    "- Functional MRI connectome data\n",
    "- Quantitative metadata (e.g., test scores)\n",
    "- Categorical metadata (e.g., demographics)\n",
    "- Targets: ADHD diagnosis & Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Connectome: (1213, 19901)\n",
      "Train Quantitative metadata: (1213, 19)\n",
      "Train Categorical metadata: (1213, 10)\n",
      "Train Targets: (1213, 3)\n",
      "Test Connectome: (304, 19901)\n",
      "Test Quantitative metadata: (304, 19)\n",
      "Test Categorical metadata: (304, 10)\n"
     ]
    }
   ],
   "source": [
    "# === Load TRAIN data ===\n",
    "train_path = \"/Users/Haley/Desktop/WiDs Datathon/widsdatathon2025/TRAIN_NEW\"\n",
    "connectome_train = pd.read_csv(f\"{train_path}/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv\")\n",
    "quant_meta_train = pd.read_excel(f\"{train_path}/TRAIN_QUANTITATIVE_METADATA_new.xlsx\")\n",
    "cat_meta_train = pd.read_excel(f\"{train_path}/TRAIN_CATEGORICAL_METADATA_new.xlsx\")\n",
    "targets_train = pd.read_excel(f\"{train_path}/TRAINING_SOLUTIONS.xlsx\")\n",
    "\n",
    "\n",
    "# Check shapes\n",
    "print(\"Train Connectome:\", connectome_train.shape)\n",
    "print(\"Train Quantitative metadata:\", quant_meta_train.shape)\n",
    "print(\"Train Categorical metadata:\", cat_meta_train.shape)\n",
    "print(\"Train Targets:\", targets_train.shape)\n",
    "\n",
    "# === Load TEST data ===\n",
    "test_path = \"/Users/Haley/Desktop/WiDs Datathon/widsdatathon2025/TEST\"\n",
    "connectome_test = pd.read_csv(f\"{test_path}/TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv\")\n",
    "quant_meta_test = pd.read_excel(f\"{test_path}/TEST_QUANTITATIVE_METADATA.xlsx\")\n",
    "cat_meta_test = pd.read_excel(f\"{test_path}/TEST_CATEGORICAL.xlsx\")\n",
    "\n",
    "\n",
    "# Check shapes\n",
    "print(\"Test Connectome:\", connectome_test.shape)\n",
    "print(\"Test Quantitative metadata:\", quant_meta_test.shape)\n",
    "print(\"Test Categorical metadata:\", cat_meta_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200)\n"
     ]
    }
   ],
   "source": [
    "def flatten_to_square_matrix(flattened_fcm, size=200):\n",
    "    # Ensure the length of the flattened matrix corresponds to the upper triangular part of a 192x192 matrix\n",
    "    num_elements = len(flattened_fcm)\n",
    "    expected_elements = (size * (size - 1)) // 2\n",
    "\n",
    "    if num_elements != expected_elements:\n",
    "        raise ValueError(f\"Flattened matrix size mismatch. Expected {expected_elements} elements, got {num_elements}\")\n",
    "\n",
    "    # Initialize a square matrix (size x size) filled with zeros\n",
    "    matrix = np.zeros((size, size))\n",
    "\n",
    "    # Extract the upper triangular indices (i, j) where i < j\n",
    "    indices = np.triu_indices(size, k=1)  # k=1 excludes diagonal (i != j)\n",
    "\n",
    "    # Assign the flattened values to the upper triangular part of the matrix\n",
    "    matrix[indices] = flattened_fcm\n",
    "    matrix.T[indices] = flattened_fcm  # Symmetric part: Copy to the lower triangle\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# Example for the first participant\n",
    "flattened_fcm = connectome_train.iloc[0, 1:].values  # Skip the participant_id column\n",
    "fcm_matrix = flatten_to_square_matrix(flattened_fcm)\n",
    "print(fcm_matrix.shape)  # Should print (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Connectomes: (1213, 200, 200)\n",
      "Test Connectomes: (304, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_FCM is a pandas DataFrame with participant IDs and flattened FCMs\n",
    "connectivity_matrices = []\n",
    "\n",
    "for i in range(len(connectome_train)):\n",
    "    flattened_fcm = connectome_train.iloc[i, 1:].values  # Skip the participant_id column\n",
    "    fcm_matrix = flatten_to_square_matrix(flattened_fcm)\n",
    "    connectivity_matrices.append(fcm_matrix)\n",
    "\n",
    "connectivity_matrices = np.array(connectivity_matrices)\n",
    "print(\"Train Connectomes:\",connectivity_matrices.shape)  # Should print (N, 200, 200), where N is the number of participants\n",
    "\n",
    "\n",
    "connectivity_matrices_test = []\n",
    "\n",
    "for i in range(len(connectome_test)):\n",
    "    flattened_fcm = connectome_test.iloc[i, 1:].values  # Skip the participant_id column\n",
    "    fcm_matrix = flatten_to_square_matrix(flattened_fcm)\n",
    "    connectivity_matrices_test.append(fcm_matrix)\n",
    "\n",
    "connectivity_matrices_test = np.array(connectivity_matrices_test)\n",
    "print(\"Test Connectomes:\",connectivity_matrices_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add channel: inputs of shape (batch_size, 1, d, d) â€” that \"1\" is because CNNs expect a channel dimension (like a color channel in images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metadata: (1213, 33)\n",
      "Test metadata: (304, 33)\n",
      "Columns: Index(['PreInt_Demos_Fam_Child_Ethnicity', 'PreInt_Demos_Fam_Child_Race',\n",
      "       'MRI_Track_Scan_Location', 'Barratt_Barratt_P1_Edu',\n",
      "       'Barratt_Barratt_P1_Occ', 'Barratt_Barratt_P2_Edu',\n",
      "       'Barratt_Barratt_P2_Occ', 'Basic_Demos_Enroll_Year_2016',\n",
      "       'Basic_Demos_Enroll_Year_2017', 'Basic_Demos_Enroll_Year_2018',\n",
      "       'Basic_Demos_Enroll_Year_2019', 'Basic_Demos_Enroll_Year_2020',\n",
      "       'Basic_Demos_Study_Site_2', 'Basic_Demos_Study_Site_3',\n",
      "       'Basic_Demos_Study_Site_4', 'EHQ_EHQ_Total', 'ColorVision_CV_Score',\n",
      "       'APQ_P_APQ_P_CP', 'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV',\n",
      "       'APQ_P_APQ_P_OPD', 'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP',\n",
      "       'SDQ_SDQ_Conduct_Problems', 'SDQ_SDQ_Difficulties_Total',\n",
      "       'SDQ_SDQ_Emotional_Problems', 'SDQ_SDQ_Externalizing',\n",
      "       'SDQ_SDQ_Generating_Impact', 'SDQ_SDQ_Hyperactivity',\n",
      "       'SDQ_SDQ_Internalizing', 'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial',\n",
      "       'MRI_Track_Age_at_Scan'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Metadata (numeric) ---\n",
    "# Socio-demographic and questionnaire data (after preprocessing, e.g., mean-imputation + one-hot encoding)\n",
    "# Shape: (n_subjects, n_features)\n",
    "train_metadata = pd.read_csv(\"/Users/Haley/Desktop/WiDs Datathon/WiDs Notebooks/meta_train.csv\")\n",
    "train_metadata = train_metadata.drop(columns = ['participant_id'])\n",
    "print(\"Train metadata:\",train_metadata.shape)\n",
    "# Same for test set\n",
    "test_metadata = pd.read_csv(\"/Users/Haley/Desktop/WiDs Datathon/WiDs Notebooks/meta_test.csv\")\n",
    "test_metadata = test_metadata.drop(columns = ['participant_id'])\n",
    "print(\"Test metadata:\",test_metadata.shape)\n",
    "print(\"Columns:\",train_metadata.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP layer for metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataBranch(nn.Module):\n",
    "    def __init__(self, input_dim=33, hidden_dims=[64, 32], dropout_rate=0.3):\n",
    "        super(MetadataBranch, self).__init__()\n",
    "        \n",
    "        # First dense layer: 33 -> 64\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Second dense layer: 64 -> 32\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Output: metadata latent vector (size 32 here)\n",
    "        self.output_dim = hidden_dims[-1]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge-to-Edge layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class E2EBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, kernel_size1, kernel_size2, bias=False):\n",
    "        super(E2EBlock, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_planes, planes, kernel_size=kernel_size1, bias=bias)\n",
    "        self.cnn2 = nn.Conv2d(in_planes, planes, kernel_size=kernel_size2, bias=bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = self.cnn1(x)  # (batch, planes, height, 1)\n",
    "        b = self.cnn2(x)  # (batch, planes, 1, width)\n",
    "        \n",
    "        # Match the dimensions by repeating\n",
    "        a = a.repeat(1, 1, 1, b.shape[3])  # repeat along width\n",
    "        b = b.repeat(1, 1, a.shape[2], 1)  # repeat along height\n",
    "\n",
    "        return a + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class E2EBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, kernel_size1, kernel_size2, bias=False):\n",
    "        \"\"\"\n",
    "        Flexible E2EBlock to allow different kernel sizes.\n",
    "        \n",
    "        Args:\n",
    "        - in_planes: input channels (1 for first layer, 32 for second)\n",
    "        - planes: output channels\n",
    "        - kernel_size1: kernel size for first conv (e.g., (1, d) or (1, 1))\n",
    "        - kernel_size2: kernel size for second conv (e.g., (d, 1) or (1, 1))\n",
    "        \"\"\"\n",
    "        super(E2EBlock, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_planes, planes, kernel_size=kernel_size1, bias=bias)\n",
    "        self.cnn2 = nn.Conv2d(in_planes, planes, kernel_size=kernel_size2, bias=bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = self.cnn1(x)  # conv1 output\n",
    "        b = self.cnn2(x)  # conv2 output\n",
    "\n",
    "        # Expand spatial dimensions to match before summing\n",
    "        a = a.expand(-1, -1, b.shape[2], b.shape[3])\n",
    "        b = b.expand(-1, -1, a.shape[2], a.shape[3])\n",
    "\n",
    "        return a + b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BrainNetCNN Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(input/output layer table goes here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainNetCNNWithMetadata(nn.Module):\n",
    "    def __init__(self, example, metadata_dim, num_classes=2):\n",
    "        super(BrainNetCNNWithMetadata, self).__init__()\n",
    "        d = example.size(3)  # connectome dimension (dxd)\n",
    "\n",
    "        # Brain branch\n",
    "        self.e2econv1 = E2EBlock(1, 32, kernel_size1=(1, d), kernel_size2=(d, 1))\n",
    "        self.e2econv2 = E2EBlock(32, 64, kernel_size1=(1, 1), kernel_size2=(1, 1))\n",
    "\n",
    "        self.e2n = nn.Conv2d(64, 1, kernel_size=(1,d))\n",
    "        self.n2g = nn.Conv2d(1, 256, kernel_size=(d, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 30)\n",
    "\n",
    "        # Metadata branch\n",
    "        self.metadata_fc1 = nn.Linear(metadata_dim, 64)\n",
    "        self.metadata_fc2 = nn.Linear(64, 30)\n",
    "\n",
    "        # Combined final layer\n",
    "        self.fc_combined = nn.Linear(30 + 30, num_classes)\n",
    "\n",
    "        # Common stuff\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.act = nn.LeakyReLU(negative_slope=0.33)\n",
    "\n",
    "    def forward(self, brain_x, metadata_x):\n",
    "        # Brain branch\n",
    "        x = self.act(self.e2econv1(brain_x)) \n",
    "        x = self.act(self.e2econv2(x))\n",
    "        x = self.act(self.e2n(x))\n",
    "        x = self.dropout(self.act(self.n2g(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.dropout(self.act(self.fc1(x)))\n",
    "        x = self.dropout(self.act(self.fc2(x)))  # Shape: (batch, 30)\n",
    "\n",
    "        # Metadata branch\n",
    "        m = self.dropout(self.act(self.metadata_fc1(metadata_x)))\n",
    "        m = self.dropout(self.act(self.metadata_fc2(m)))             # Shape: (batch, 30)\n",
    "\n",
    "        # Combine\n",
    "        combined = torch.cat([x, m], dim=1)  # Shape: (batch, 60)\n",
    "\n",
    "        # Final prediction\n",
    "        out = self.fc_combined(combined)     # Shape: (batch, num_classes)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function: Binary Cross-entropy with logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, connectomes_np, metadata_df, targets_df):\n",
    "        \"\"\"\n",
    "        connectomes_np: numpy array of shape (N_samples, nodes, nodes)\n",
    "        metadata_df: pandas DataFrame of shape (N_samples, num_meta_features)\n",
    "        targets_df: pandas DataFrame of shape (N_samples, 2)\n",
    "        \"\"\"\n",
    "        self.connectomes = torch.from_numpy(connectomes_np).float()\n",
    "        self.metadata = torch.from_numpy(metadata_df.to_numpy()).float()\n",
    "        self.targets = torch.from_numpy(targets_df).float()\n",
    "        \n",
    "        # Add channel dimension if missing\n",
    "        if self.connectomes.ndim == 3:\n",
    "            self.connectomes = self.connectomes.unsqueeze(1)  # (N_samples, 1, nodes, nodes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.connectomes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        connectome = self.connectomes[idx]      # Tensor (1, nodes, nodes)\n",
    "        metadata = self.metadata[idx]            # Tensor (num_meta_features,)\n",
    "        target = self.targets[idx]               # Tensor (2,)\n",
    "        return connectome, metadata, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert target_train to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       ...,\n",
       "       [0, 1],\n",
       "       [0, 0],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_train = pd.read_excel(f\"{train_path}/TRAINING_SOLUTIONS.xlsx\")\n",
    "targets_train = targets_train[[\"ADHD_Outcome\",\"Sex_F\"]] # drop participant ID\n",
    "targets_train1 = targets_train.to_numpy()\n",
    "targets_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "dataset = BrainDataset(connectivity_matrices, train_metadata, targets_train1)\n",
    "\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# In your training loop:\n",
    "# for connectome_batch, metadata_batch, target_batch in dataloader:\n",
    "    # connectome_batch -> (32, 1, 82, 82)\n",
    "    # metadata_batch -> (32, 33)\n",
    "    # target_batch -> (32, 2)\n",
    "    # ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "brain_x, metadata_x, targets = next(iter(dataloader))  # get one batch\n",
    "brain_x\n",
    "model = BrainNetCNNWithMetadata(brain_x, metadata_dim=33, num_classes=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for brain_x, metadata_x, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(brain_x, metadata_x)  # (batch_size, 2)\n",
    "        \n",
    "        loss = loss_fn(outputs, targets.float())  # IMPORTANT: targets must be float (not long)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} - Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn outputs (raw logits) into probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(brain_x, metadata_x)\n",
    "probs = torch.sigmoid(outputs)  # (batch_size, 2)\n",
    "\n",
    "adhd_probs = probs[:, 0]  # probability of ADHD\n",
    "sex_probs = probs[:, 1]   # probability of Male (or whatever label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold set @ 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhd_preds = (adhd_probs > 0.5).int()\n",
    "sex_preds = (sex_probs > 0.5).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'connectivity_matrices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- 1. Functional connectome matrices (your graphs) ---\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# List or array of shape [n_subjects, n_nodes, n_nodes]\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m train_connectomes \u001b[38;5;241m=\u001b[39m  connectivity_matrices \u001b[38;5;66;03m# shape (n_subjects, n_nodes, n_nodes)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m test_connectomes \u001b[38;5;241m=\u001b[39m connectivity_matrices_test   \u001b[38;5;66;03m# same for test\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# --- 2. Target Variables (binary) ---\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ADHD diagnosis labels: 0 = no ADHD, 1 = ADHD\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'connectivity_matrices' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 1. Functional connectome matrices (your graphs) ---\n",
    "# List or array of shape [n_subjects, n_nodes, n_nodes]\n",
    "train_connectomes =  connectivity_matrices # shape (n_subjects, n_nodes, n_nodes)\n",
    "test_connectomes = connectivity_matrices_test   # same for test\n",
    "\n",
    "# --- 2. Target Variables (binary) ---\n",
    "# ADHD diagnosis labels: 0 = no ADHD, 1 = ADHD\n",
    "adhd_labels = targets_train['ADHD_Outcome']\n",
    "sex_labels = targets_train['Sex_F'] # shape (n_subjects,)\n",
    "\n",
    "# --- 4. Participant IDs (optional, for submission file) ---\n",
    "# This is just the list of unique participant IDs, e.g., \"v1nMpCoLGU0V\"\n",
    "# test_participant_ids = np.load('path_to_your_test_participant_ids.npy')  # shape (n_test_subjects,)\n",
    "\n",
    "all_columns = list(set(train_metadata.columns) | set(test_metadata.columns))\n",
    "print(len(test_metadata.columns))\n",
    "# --- 5. Normalize Metadata ---\n",
    "# Always normalize metadata before feeding into model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_metadata = scaler.fit_transform(train_metadata)\n",
    "test_metadata = scaler.transform(test_metadata)\n",
    "\n",
    "\n",
    "# --- 6. Create Datasets and DataLoaders ---\n",
    "train_dataset = ConnectomeDataset(train_connectomes, adhd_labels, sex_labels, train_metadata)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# test_dataset = ConnectomeDataset(test_connectomes, np.zeros(len(test_connectomes)), np.zeros(len(test_connectomes)), test_metadata)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_edges(matrix, threshold=0.0):\n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if matrix[i, j] > threshold and i != j:\n",
    "                edge_index.append([i, j])\n",
    "                edge_weight.append(matrix[i, j])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "def compute_node_features(matrix):\n",
    "    degree = matrix.sum(axis=1)\n",
    "    node_features = torch.tensor(degree, dtype=torch.float).unsqueeze(1)  # [n_nodes, 1]\n",
    "    return node_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "class ConnectomeDataset(Dataset):\n",
    "    def __init__(self, connectomes, adhd_labels, sex_labels, metadata, threshold=0.0):\n",
    "        super().__init__()\n",
    "        self.connectomes = connectomes\n",
    "        self.adhd_labels = adhd_labels\n",
    "        self.sex_labels = sex_labels\n",
    "        self.metadata = metadata\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.connectomes)\n",
    "\n",
    "    def get(self, idx):\n",
    "        matrix = self.connectomes[idx]\n",
    "        edge_index, edge_weight = matrix_to_edges(matrix, threshold=self.threshold)\n",
    "\n",
    "        x = compute_node_features(matrix)  # now using degree features\n",
    "\n",
    "        y_adhd = torch.tensor(self.adhd_labels[idx], dtype=torch.long)\n",
    "        y_sex = torch.tensor(self.sex_labels[idx], dtype=torch.long)\n",
    "\n",
    "        meta = torch.tensor(self.metadata[idx], dtype=torch.float)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight)\n",
    "        data.y_adhd = y_adhd\n",
    "        data.y_sex = y_sex\n",
    "        data.metadata = meta\n",
    "        data.num_nodes = matrix.shape[0]\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "# class GNNWithMetadata(torch.nn.Module):\n",
    "#     def __init__(self, hidden_channels, metadata_dim):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = GCNConv(1, hidden_channels)  # input dim is 1 (degree feature)\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "#         self.meta_mlp = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(metadata_dim),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(1, hidden_channels)\n",
    "#         )\n",
    "\n",
    "#         self.lin_adhd = torch.nn.Linear(hidden_channels + 32, 2)\n",
    "#         self.lin_sex = torch.nn.Linear(hidden_channels + 32, 2)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch, metadata):\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "\n",
    "#         x = global_mean_pool(x, batch)\n",
    "\n",
    "#         meta_embedding = self.meta_mlp(metadata)\n",
    "\n",
    "#         fused = torch.cat([x, meta_embedding], dim=1)\n",
    "\n",
    "#         adhd_out = self.lin_adhd(fused)\n",
    "#         sex_out = self.lin_sex(fused)\n",
    "\n",
    "#         return adhd_out, sex_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNWithMetadata(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, metadata_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_channels)  # input dim is 1 (degree feature)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.meta_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(metadata_dim, hidden_channels),  # Fix metadata input size\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels)  # Hidden size matches GCN output\n",
    "        )\n",
    "\n",
    "        self.lin_adhd = torch.nn.Linear(hidden_channels + hidden_channels, 2)\n",
    "        self.lin_sex = torch.nn.Linear(hidden_channels + hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, metadata):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)  # Global pooling of node features\n",
    "\n",
    "        metadata = metadata.view(metadata.size(0), -1)  # Flatten if needed\n",
    "        meta_embedding = self.meta_mlp(metadata)\n",
    "\n",
    "        fused = torch.cat([x, meta_embedding], dim=1)\n",
    "\n",
    "        adhd_out = self.lin_adhd(fused)\n",
    "        sex_out = self.lin_sex(fused)\n",
    "\n",
    "        return adhd_out, sex_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        adhd_out, sex_out = model(data.x, data.edge_index, data.batch, data.metadata)\n",
    "\n",
    "        loss_adhd = F.cross_entropy(adhd_out, data.y_adhd)\n",
    "        loss_sex = F.cross_entropy(sex_out, data.y_sex)\n",
    "\n",
    "        loss = loss_adhd + loss_sex\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    adhd_preds = []\n",
    "    sex_preds = []\n",
    "    participant_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            adhd_out, sex_out = model(data.x, data.edge_index, data.batch, data.metadata)\n",
    "\n",
    "            adhd_probs = F.softmax(adhd_out, dim=1)[:, 1]\n",
    "            sex_probs = F.softmax(sex_out, dim=1)[:, 1]\n",
    "\n",
    "            adhd_preds.append(adhd_probs.cpu())\n",
    "            sex_preds.append(sex_probs.cpu())\n",
    "            participant_ids.extend(data.participant_id)  # if available\n",
    "\n",
    "    adhd_preds = torch.cat(adhd_preds).numpy()\n",
    "    sex_preds = torch.cat(sex_preds).numpy()\n",
    "\n",
    "    return participant_ids, adhd_preds, sex_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1056x1 and 33x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m51\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train(model, train_loader, optimizer, device)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear previous gradients\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Forward pass: compute predicted outputs by passing inputs to the model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m adhd_out, sex_out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch, data\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m adhd_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(adhd_out, data\u001b[38;5;241m.\u001b[39my[:, \u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# ADHD label is the first column of y\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[54], line 25\u001b[0m, in \u001b[0;36mGNNWithMetadata.forward\u001b[0;34m(self, x, edge_index, batch, metadata)\u001b[0m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m global_mean_pool(x, batch)  \u001b[38;5;66;03m# Global pooling of node features\u001b[39;00m\n\u001b[1;32m     24\u001b[0m metadata \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mview(metadata\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten if needed\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m meta_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_mlp(metadata)\n\u001b[1;32m     27\u001b[0m fused \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, meta_embedding], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m adhd_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_adhd(fused)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1056x1 and 33x128)"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "metadata_dim = train_metadata.shape[1]\n",
    "\n",
    "model = GNNWithMetadata(hidden_channels=128, metadata_dim=metadata_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "train_dataset = ConnectomeDataset(train_connectomes, adhd_labels, sex_labels, train_metadata)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    loss = train(model, train_loader, optimizer, device)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metadata.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, loader, optimizer, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)  # Move data to the appropriate device\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        adhd_out, sex_out = model(data.x, data.edge_index, data.batch, data.metadata)\n",
    "\n",
    "        # Compute the loss\n",
    "        adhd_loss = F.cross_entropy(adhd_out, data.y[:, 0])  # ADHD label is the first column of y\n",
    "        sex_loss = F.cross_entropy(sex_out, data.y[:, 1])    # Sex label is the second column of y\n",
    "        loss = adhd_loss + sex_loss  # Combine both losses\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    return total_loss / len(loader)  # Return the average loss for this epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1056 and 32x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model for 50 epochs\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m51\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train(model, train_loader, optimizer, device)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear previous gradients\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Forward pass: compute predicted outputs by passing inputs to the model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m adhd_out, sex_out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch, data\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m adhd_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(adhd_out, data\u001b[38;5;241m.\u001b[39my[:, \u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# ADHD label is the first column of y\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[50], line 32\u001b[0m, in \u001b[0;36mGNNWithMetadata.forward\u001b[0;34m(self, x, edge_index, batch, metadata)\u001b[0m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m global_mean_pool(x, batch)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Apply MLP to the metadata\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m meta_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_mlp(metadata)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Concatenate the GNN output and metadata embedding\u001b[39;00m\n\u001b[1;32m     35\u001b[0m fused \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, meta_embedding], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1056 and 32x16)"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = GNNWithMetadata().to(device)  # Removed the hidden_channels parameter\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Assuming 'train_connectomes', 'adhd_labels', 'sex_labels', 'train_metadata' are pre-defined\n",
    "train_dataset = ConnectomeDataset(train_connectomes, adhd_labels, sex_labels, train_metadata)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train the model for 50 epochs\n",
    "for epoch in range(1, 51):\n",
    "    loss = train(model, train_loader, optimizer, device)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def compute_weighted_f1(y_true_adhd, y_pred_adhd, y_true_sex, y_pred_sex, sex_true):\n",
    "    # Normal F1 scores\n",
    "    _, _, f1_adhd, _ = precision_recall_fscore_support(y_true_adhd, y_pred_adhd, average='binary')\n",
    "    _, _, f1_sex, _ = precision_recall_fscore_support(y_true_sex, y_pred_sex, average='binary')\n",
    "\n",
    "    # Now handle special weighting: 2x weight for Female ADHD cases\n",
    "    special_cases = (y_true_adhd == 1) & (sex_true == 1)\n",
    "    if special_cases.sum() > 0:\n",
    "        # Compute precision, recall, f1 only on female ADHD cases\n",
    "        special_precision, special_recall, special_f1, _ = precision_recall_fscore_support(\n",
    "            y_true_adhd[special_cases], y_pred_adhd[special_cases], average='binary'\n",
    "        )\n",
    "        # Boost f1_adhd by averaging\n",
    "        f1_adhd = (f1_adhd + special_f1) / 2\n",
    "\n",
    "    final_score = (f1_adhd + f1_sex) / 2\n",
    "\n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "metadata_dim = train_metadata.shape[1]\n",
    "\n",
    "model = GNNWithMetadata(hidden_channels=128, metadata_dim=metadata_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# train_dataset = ConnectomeDataset(train_connectomes, train_adhd_labels, train_sex_labels, train_metadata)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    loss = train(model, train_loader, optimizer, device)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m connectome_data_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(connectivity_matrices, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add channel dimension\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Ensure the tabular data is also in tensor format\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_X_tabular_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(train_metadata\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming Y_train_tensor is the target tensor (ADHD and Sex)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m Y_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(targets_train\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming connectivity_matrices is a numpy array with shape (N, 192, 192)\n",
    "connectome_data_tensor = torch.tensor(connectivity_matrices, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "# Ensure the tabular data is also in tensor format\n",
    "train_X_tabular_tensor = torch.tensor(train_metadata.values, dtype=torch.float32)\n",
    "\n",
    "# Assuming Y_train_tensor is the target tensor (ADHD and Sex)\n",
    "Y_train_tensor = torch.tensor(targets_train.values, dtype=torch.float32)\n",
    "\n",
    "# Now create the TensorDataset\n",
    "train_dataset = TensorDataset(connectome_data_tensor, train_X_tabular_tensor, Y_train_tensor)\n",
    "\n",
    "# Create the DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "class CCNN(nn.Module):\n",
    "    def __init__(self, num_tabular_features, num_classes=2):\n",
    "        super(CCNN, self).__init__()\n",
    "        \n",
    "        # 2D Convolutional layers for the connectome data (200x200 images)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  # Output: (32, 200, 200)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)       # Output: (32, 100, 100)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) # Output: (64, 100, 100)\n",
    "        \n",
    "        # Fully connected layers for the tabular data\n",
    "        self.fc1 = nn.Linear(num_tabular_features, 128)\n",
    "        \n",
    "        # Final fully connected layer to output predictions for ADHD and Sex\n",
    "        # Adjusting the input size after flattening the output of convolution layers\n",
    "        self.fc2 = nn.Linear(64 * 50 * 50 + 128, num_classes)  # Flattened output size: 64 * 50 * 50 = 160000\n",
    "\n",
    "    def forward(self, x_connectome, x_tabular):\n",
    "        # Apply convolutional layers on the connectome data\n",
    "        x = self.pool(F.relu(self.conv1(x_connectome)))  # Output: (32, 100, 100)\n",
    "        x = self.pool(F.relu(self.conv2(x)))             # Output: (64, 50, 50)\n",
    "        \n",
    "        # Flatten the 2D output of the convolutional layers\n",
    "        x_connectome_flat = x.view(x.size(0), -1)  # Flatten to (N, 64*50*50)\n",
    "        \n",
    "        # Pass tabular data through the fully connected layer\n",
    "        x_tabular = F.relu(self.fc1(x_tabular))\n",
    "        \n",
    "        # Concatenate the flattened connectome data with the tabular data\n",
    "        x_combined = torch.cat((x_connectome_flat, x_tabular), dim=1)\n",
    "        \n",
    "        # Final fully connected layer for classification\n",
    "        x = self.fc2(x_combined)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = CCNN(num_tabular_features=train_X_tabular.shape[1])\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10  # You can adjust this\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    for batch_idx, (connectome_data, tabular_data, labels) in enumerate(train_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(connectome_data, tabular_data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_preds += labels.size(0)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Print statistics\n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {correct_preds/total_preds:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wids-datathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
